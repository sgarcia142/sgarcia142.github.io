---
title: "STA6443 Final Exam"
author: "Stephen Garcia (wqr974)"
date: "November 21, 2024"
format: gfm
---

```{r}
#| label = "setup",
#| include = FALSE

pacman::p_load(tidyverse, ggtext, png, jpeg, showtext, ggcorrplot, corrplot, GGally, scales, gt, patchwork, car, viridis, AICcmodavg, dplyr, reshape2,dlookr,e1071,caret,caTools, smotefamily, leaps, vcd, ggalluvial,cowplot, ggpubr,ggeffects,modelr)
theme_set(theme_minimal())
```

# Final Exam
## Part 1: Definition (20 Points)
Define the following terms and where appropriate compare and contrast with the alternative method.

### EDA
Exploratory data analysis (EDA) is a critical step in the data analysis process.  During this step, the analyst will examine the data set to summarize its main characteristics, identify patters, detect anomalies, and formulate hypotheses. EDA is the catalyst for applying statistical modeling or machine learning algorithms and ensures the quality and relevance of the data for analysis.

*Specific Tasks in EDA*\

1. Understand the dataset
    + Check the metadata
    + Examine the data types
    + Inspect the structure
    
2. Handle Missing data
    + Check for missing values: Quantify missingness and determine patterns
    + Decide on a strategy
      - Drop rows/columns with excessive missing data
      - Impute missing values
      
3. Univariate Analysis
    + Numerical variables
      - Summary statistics: mean, median, standard deviation, skewness
      - Visualizations: histograms, boxplots
    + Categorical variables
      - Frequency tables and bar plots
      - Check for imbalances in categories
      
4. Multivariate Analysis
    + Numerical vs. Numerical
      - Correlation matrix 
      - Scatter plots or pair plots
    + Categorical vs. Numerical
      - Boxplots, violin plots
    + Categorical vs. Categorical
      - Cross-tabulation or contingency tables
      - Stacked bar charts
      
5. Identify Outliers
    + Detect outliers
      - Boxplots
      - Z-scores
      - Interquartile range (IQR): Identify values outside 1.5 * IQR
    + Decide on handling
      - Keep, transform, or remove outliers depending on the context
      
 6. Check for Data Distribution
    + Understand how data is distributed to make statistical inferences
      - Histograms for overall shape 
      - Q-Q plots to check normality
      - Transformations if normality is required for modeling
      
 7. Feature Engineering Insights
    + Combine existing variables
    + Extract features
    + One-hot encode categorical variables for models requiring numerical inputs

 8. Check for Multicollinearity
    + Identify highly correlated independent variables
      - Use correlation heatmaps
      - Variance Inflation Factor (VIF): Flag features with high VIF (>5 or 10)
      
 9. Document Findings
    + Summarize insights, questions, and decisions in a reproducible format
    + Write clear markdown or reports with visualizations
    
### One-Way ANOVA versus Two-Way ANOVA
 1. One-Way ANOVA 
    + Tests whether there are statistically significant difference in the means of 3 or more groups for a single independent variable.
    + Assumptions
      - Independence of Observations
      - Normality (Shapiro-Wilk test): relatively robust to moderate deviations from normality
      - Homoscedasticity (Levene's test): variance should be equal across all groups
      - Continuous dependent variable
    + The Null Hypothesis (HO): Assumes that there are no significant differences between the group means
    + Alternative Hypothesis (H1): Assumes that at least one group mean is significantly different from the others; however, it doesn't tell us which group.  In order to determine that, you will need to perform some post-hoc tests.

 2. Two-Way ANOVA
    + Tests the effect of two independent categorical variables on a single continuous dependent variable, including the interaction between the two independent variables. 
    + Assumptions are the same as One-Way
    + Interaction Effect: determines whether the effect of one factor depends on the levels of the other factor.

*Example*
For a One-Way ANOVA you could compare the test scores of students across three or more different teaching methods; whereas, a Two-Way ANOVA could compare test scores based on teaching method and the modality of the course (whether it be online or in person), including whether the teaching method performs differently in each classroom type.

### The method of least squares for regression 
The method of least squares is a fundamental statistical technique used in regression analysis to find the best-fitting line or curve for a set of observed data. It does this by minimizing the sum of the squares of the vertical distances (residuals) between the observed values and the values predicted by the model. When calculated, the resulting regression line or curve is considered the "best fit" in the least squares sense because it has the smallest possible sum of squared differences from the observed data.

### Assumptions of a linear model and how to check them
 1. Linearity: The relationship between the independent variables (predictors) and the dependent variable (response) is linear. 
    + Plot the residuals versus the predicted values for each independent variable. If the relationship is linear, the residuals should be randomly scattered around zero without any discernible pattern.
2. Homoscedasticity: The residuals have constant variance at every level of the independent variables. This means that the spread of the residuals should be the same across all levels of the predictors.
    + Residuals vs. Predicted Values Plot
3. Normality: The residuals are normally distributed. This assumption is essential for hypothesis testing and constructing confidence intervals.
    + Histogram or Q-Q Plot of Residuals
    + Shapiro-Wilk Test
    + Skewness Measures: Quantify deviations from normality.
4. No Multicollinearity: High multicollinearity can inflate the variance of coefficient estimates and make them unstable.
    + Variance Inflation Factor (VIF)
    + Correlation Matrix

## Part 2: Regression (40 Points)
1. Read in the data
```{r}
cat.o <- read_csv("catalog.csv", show_col_types = FALSE)
```

2. Factorize Features
    + We want to ensure that these features are recognized as discrete categorical predictors.
```{r}
# Covert to factors
cat.o$CollGifts<-as_factor(cat.o$CollGifts)
cat.o$BricMortar<-as_factor(cat.o$BricMortar)
cat.o$MarthaHome<-as_factor(cat.o$MarthaHome)
cat.o$SunAds<-as_factor(cat.o$SunAds)
cat.o$ThemeColl<-as_factor(cat.o$ThemeColl)
cat.o$CustDec<-as_factor(cat.o$CustDec)
cat.o$RetailKids<-as_factor(cat.o$RetailKids)
cat.o$TeenWr<-as_factor(cat.o$TeenWr)
cat.o$Carlovers<-as_factor(cat.o$Carlovers)
cat.o$CountryColl<-as_factor(cat.o$CountryColl)

# Check Structure
str(cat.o)
```

3. Clean the original data
    + We were told that there were no missing data; however, I will perform this check anyway
    + Remove records with age values less than 18
    + Remove records with a length of residence greater than age
    + Consider changing Income to factor
```{r}
# Count missing values in each column 
missing_counts <- colSums(is.na(cat.o)) 
missing_counts

# Clean original dataframe by only including those observations that have age that is greater than 
# 18 and a length of residence that is less than the age.
cat.c <- filter(cat.o, Age>=18, LenRes<=Age)

# Check the cardinality of Income
distinct_values <- unique(cat.c$Income)
distinct_values
```
*Summary*\
Data cleaning is a critical preliminary step in any data analysis process. It involves identifying and correcting errors, inconsistencies, and inaccuracies in the dataset to ensure that the data is accurate, complete, and ready for analysis. For this dataset we took action on two features and considered whether to create a factor out of the ordinal feature Income. The two features we took action on were Age age and Length of Residence.  We removed those records for Age that were less than 18.  We did this because the analysis we are performing revolves around Spending Ratio and since those who are under 18 aren't going to spend, we extracted them from the data.  We also removed those observations where the Length of Residence was greater than the Age since this happenstance should never occur.  I considered changing Income to a factor, I checked the cardinality and there are only 9 unique entries; however, since the variable is inherently numeric I have decided to leave it as is. I will check to see if changing it enhances the model but that will be later in the process.

4. Basic Summary
    + Provide a basic summary of the cleaned data set
    + Include a table of univariate statistics to summarize each variable
    + Basic summary of the catalog spending (SpendRat) including an appropriate graphical display


```{r}
#| fig-width: 12
#| fig-height: 6

# Summary
summary(cat.c)

# Determine appropriate bin width
bw <- 2 * IQR(cat.c$SpendRat) / length(cat.c$SpendRat)^(1/3)

# Histogram of the Spend Rate response variable
hist_1 <- ggplot(cat.c, aes(x=SpendRat)) + 
  geom_histogram(binwidth = bw, fill = "#E46726", color = "black") + 
  geom_vline(aes(xintercept = mean(SpendRat,na.rm=TRUE)),color="black", linetype="dashed", linewidth=1) + 
  labs(
    x = "Spend Rate", 
    y = "Observations", 
    title = '<span style="font-size: 25pt;">Spend Rate for Catelog</span>',
    subtitle = '<span style="font-size: 16pt;">Positively Skewed with Outliers</span>'
  ) + 
  theme(plot.title = element_markdown(lineheight = 1.2), 
        plot.subtitle = element_markdown(lineheight = 1.2)
  )  + 
  annotate("text", x=mean(cat.c$SpendRat + 80, na.rm=TRUE)
           , y=Inf
           , label=paste("Mean =", round(mean(cat.c$SpendRat, na.rm=TRUE), 0))
           , vjust=4, color="black")

# Determine appropriate bin width for log transformed Spend Rate
bw <- 2 * IQR(log(cat.c$SpendRat)) / length(log(cat.c$SpendRat))^(1/3)

# Histogram of the Spend Rate response variable after being log transformed
hist_2 <- ggplot(cat.c, aes(x=log(SpendRat))) + 
  geom_histogram(binwidth = bw, fill = "#E46726", color = "black") + 
  geom_vline(aes(xintercept = mean(log(SpendRat),na.rm=TRUE)),color="black", 
             linetype="dashed", linewidth=1) + 
  labs(
    x = "Spend Rate", 
    y = "Observations", 
    title = '<span style="font-size: 25pt;">Spend Rate for Catelog</span>',
    subtitle = 
      '<span style="font-size: 16pt;">Spend Rate Log Transformed Generally Normal</span>'
  ) + 
  theme(plot.title = element_markdown(lineheight = 1.2), 
        plot.subtitle = element_markdown(lineheight = 1.2)
  )  + 
  annotate("text", x=mean(log(cat.c$SpendRat) + 2.5, na.rm=TRUE)
           , y=Inf
           , label=paste("Mean =", round(mean(log(cat.c$SpendRat), na.rm=TRUE), 0))
           , vjust=4, color="black")

  hist_1 + hist_2
```
5. Modeling
    + Produce a scatterplot matrix which includes all of the variables
in the data set
    + Use the lm() function to perform a multiple linear regression
with SpendRat as the response and all other variables as
the predictors. Use the summary() function to print the results.
    + Use forward, backward, or stepwise selection to find a suitable subset of this model and comment on the performance of this new model to the full model from 2.
    + Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?
    + Try a few different transformations of the variables, such as log(X),âˆšX, X^2. Comment on your findings and repeat 3 & 4.
    + Comment on the results obtained. How good do these models fit the data? Can we use any of them to predict the spending ratio? 
    
```{r}
#| fig-width: 12
#| fig-height: 12

custom_theme <- theme_bw() +
  theme(
    panel.border = element_rect(color = "black", fill = NA),
    panel.grid.major = element_line(color = "grey90"),
    panel.grid.minor = element_line(color = "grey95"),
    strip.background = element_rect(fill = "grey80", color = "grey50"),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 12)
  )


ggpairs(
  cat.c[,1:11], 
  upper = list(continuous = wrap("cor", size = 4)),
  lower = list(continuous = wrap("points", size = 1.5)),
  diag = list(continuous = wrap("densityDiag", alpha = 0.5)),
  title = "Pairs Plot of Iris Dataset"
) + 
  custom_theme +
  theme(plot.title = element_text(hjust = 0.5))
```
```{r}
# Fit model as is
fit_1 = lm(data = cat.c, SpendRat ~ .)
summary(fit_1)

# Second Fit
fit_2 = lm(data = cat.c, log(SpendRat) ~ Age  + LenRes + 
             Income + TotAsset + SecAssets + ShortLiq + 
             LongLiq + WlthIdx + SpendVol + SpenVel + CollGifts + 
             BricMortar + MarthaHome + SunAds + ThemeColl + CustDec + 
             RetailKids + TeenWr + Carlovers + CountryColl
)
summary(fit_2)
```
### First Model 
With the first model, I merely left the data alone in order to get a baseline with all of the regressors to see what their influence is on the response variable.  From the F-Statistic (3.624) and p-value (2.3e-06) in the summary we can see that there is a relationship between the regressors and the response variable; however, we can see that the model only explains approximately 30.8% of the variability. While there is a relationship, it certainly isn't a good model.

### Second Model
In the second model I am going to transform the response variable to its log. We saw early on that the Spend Rate is positively skewed and we were able to normalize it by taking its log.  Making this change did improve the overall model, the F-statistic improved to 6.54 and the p-value decreased substantially. The standard error reduced from 58.27 to 1.203.

### Perform Forward Stepwise selection.
```{r}
# Let's use best subset selection
regfit <- regsubsets(log(SpendRat)~., cat.c, nvmax=20, method ="forward")
summary(regfit)
```

### Extract the R-Squared values from the summary of the stepwise selection
```{r}
regfit.summary=summary(regfit)
regfit.summary$rsq
```

### Plot the KPI's from the summary of the stepwise selection
```{r}
#| fig-width: 12
#| fig-height: 12
par(mfrow=c(2,2))
plot(regfit.summary$rss, xlab="Number of Variables ", ylab="RSS", type="l")
plot(regfit.summary$adjr2, xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
plot(regfit.summary$cp,xlab="Number of Variables ",ylab="Cp",type='l')
plot(regfit.summary$bic ,xlab="Number of Variables ", ylab="BIC", type='l')
```
### Finalize Model
```{r}
fit_4 = lm(data = cat.c, log(SpendRat) ~ LenRes + Income + 
             ShortLiq  + SpendVol + SpenVel + CollGifts + 
             BricMortar + MarthaHome + ThemeColl + TeenWr + 
             Carlovers
)
summary(fit_4)
```

### Try to enhance final model
```{r}
fit_5 = lm(data = cat.c, log(SpendRat) ~ I(LenRes^2) + Income + 
             I(ShortLiq^2)  + SpendVol + SpenVel + CollGifts + 
             BricMortar + MarthaHome + ThemeColl + TeenWr + 
             Carlovers
)
summary(fit_5)
```
```{r}
#| fig-width: 12
#| fig-height: 8
par(mfrow=c(2,2))
plot(fit_5)
```

### Summary of final model selection
From the forward selection results, I settled on a model with 11 regressors. The R-Squared values and plots indicate diminishing returns around this number of predictors. The overall model's F-Statistic improved from 3.624 to 11.71, and the R-Squared increased to 42.82%. Although this is less than the "all regressors" version, the Adjusted R-Squared improved with each iteration, showcasing the inherent trade-offs.

After finalizing the model with 11 regressors, I explored transformations to enhance the model further. Squaring the LenRes and ShortLiq predictors increased the R-Squared to 43.86 and the F-Statistic to 12.22. The residual plots revealed some outliers, but the random distribution is a positive sign. The QQ plot confirmed that the residuals are approximately normal, indicating the model's robustness.


```{r}

```
### Comment on the results obtained
Unfortunately, none of the models we built stood out. Our final model achieved an R-Squared of 0.4386, explaining roughly 43.9% of the variance. While this might be acceptable for predictions in social science, it's less reliable for business applications. We could augment the data further and seek additional regressors, but as it stands, I am not satisfied with the results.


## Part 3: Classification (40 Points)
1. Read in the data
```{r}
adult.o<-read_csv("adult.csv", na="?", show_col_types = FALSE) 
```

### Cleaning the data 
As pointed out in the instructions, the wgt feature will be removed and I am going to convert all of the character features to factors. Also, there are missing values in the data set for the work_class, occupation, and native_country predictors.  The instructions don't specify what to do with these; however, it is best practice to handle them.  As such I will simply remove these values from the data; however, they are categorical so you could impute the values using different techniques.

```{r}
adult.o <- adult.o |> select(-wgt)
adult.c <- adult.o |> mutate_if(is.character, as.factor)
adult.c <- na.omit(adult.c)
summary(adult.c)
```

2. Explore the Data Graphically
```{r}
#| fig-width: 12
#| fig-height: 8

# Create bar plots for the categorical data
bar_1 <- ggplot(adult.c, aes(x = education, fill = income)) +
  geom_bar(position = "dodge") +
  labs(title = "Education vs. Income", x = "Education", y = "Count") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

bar_2 <- ggplot(adult.c, aes(x = work_class, fill = income)) +
  geom_bar(position = "dodge") +
  labs(title = "Work Class vs. Income", x = "Work Class", y = "Count") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

bar_3 <- ggplot(adult.c, aes(x = marital_status, fill = income)) +
  geom_bar(position = "dodge") +
  labs(title = "Marital Status vs. Income", x = "Marital Status", y = "Count") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

bar_4 <- ggplot(adult.c, aes(x = race, fill = income)) +
  geom_bar(position = "dodge") +
  labs(title = "Race vs. Income", x = "Race", y = "Count") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display charts using cowplot
ggarrange(bar_1, bar_2, bar_3, bar_4, ncol = 2, nrow = 2, common.legend = TRUE, legend = "bottom")
```
```{r}
#| fig-width: 12
#| fig-height: 8

# Take a sample as there are too many observations to make heads or tails of
set.seed(12)
df_sample <- adult.c |> slice_sample(n = 1000)

# Create box plots for numeric vs response variable
box_1 <- ggplot(df_sample, aes(x = income, y = age)) +
  geom_boxplot(outlier.shape = NA, width = 0.6, alpha = 0.7) +
  geom_jitter(width = 0.2, size = 1.5, alpha = 0.6, color = "blue") +
  labs(title = "Age vs. Income", x = "Income", y = "Age") 

box_2 <- ggplot(df_sample, aes(x = income, y = education_num)) +
  geom_boxplot(outlier.shape = NA, width = 0.6, alpha = 0.7) +
  geom_jitter(width = 0.2, size = 1.5, alpha = 0.6, color = "blue") +
  labs(title = "Education vs. Income", x = "Income", y = "Age") 

box_3 <- ggplot(df_sample, aes(x = income, y = hours_per_week)) +
  geom_boxplot(outlier.shape = NA, width = 0.6, alpha = 0.7) +
  geom_jitter(width = 0.2, size = 1.5, alpha = 0.6, color = "blue") +
  labs(title = "Hours Worked vs. Income", x = "Income", y = "Age") 

# Display charts using cowplot
ggarrange(box_1, box_2, box_3, ncol = 2, nrow = 2, common.legend = TRUE, legend = "bottom")
```

### Summary
For comparing the categorical variables, I used bar charts to visualize the relationships with income. From the education bar chart you can see that when an individual has an advanced degree they are more likely to have a income greater than 50k.  That is further explified in the scatter plot where we can see that the median income for greater than 50k is over  12 years of education.  Age is also an indicator of increased wages.  For the model, I think I will choose age, work_class, education, occupation, hours_per_week, and native_country.

3. Perform logistic regression
```{r}
lfit <- glm(income ~ age + work_class + education_num + occupation + hours_per_week, 
            data = adult.c, family = binomial)
summary(lfit)
```
```{r}
prediction_ed<-ggpredict(lfit, terms="hours_per_week [all]")
plot_ed<-plot(prediction_ed)
plot_ed

```

```{r}
exp(0.037547)
```
### Summary
The coefficient for hours worked per week is 0.037547, which represents the change in the log-odds for a one unit change in the predictor variable.  Since it is positive it will increase the likelihood of the outcome and I should expect an odds ratio greater than 1. I calculated the odds ratio by exponentiation of the Beta Coefficient and got a value of 1.038261.  We interpret this by saying that for every unit increase in hours worked the odds of receiving a salary greater than 50k increases by a factor of 1.04. I think I understand it better by saying that for every unit increase in hours worked the odds of earning a salary greater than 50k increase by 4%.

Thanks for a great semester, hope to see you in future classes!
